# Техническое видение проекта

## Технологии

Для реализации проекта будут использоваться следующие технологии:

-   **Язык программирования:** Python (актуальная стабильная версия). Выбор обусловлен простотой, обширной экосистемой библиотек для работы с LLM и асинхронным программированием.
-   **Управление зависимостями:** `uv`. Современный и быстрый инструмент для управления зависимостями Python-проектов.
-   **Взаимодействие с LLM:** `openai` клиент. Будет использоваться для работы с LLM через провайдер Openrouter.
-   **Сборка проекта:** `make`. Для автоматизации рутинных задач, таких как установка зависимостей, запуск тестов, сборка и развертывание.
-   **Telegram Bot API:** `aiogram`. Фреймворк для разработки Telegram-ботов на Python. Будет использоваться метод `polling` для обработки входящих сообщений.

## Принцип разработки

Проект будет развиваться итеративно, сфокусировано на быстрой проверке гипотез и минимальной функциональности (MVP).

*   **Итеративная разработка:** Разработка будет вестись небольшими циклами, с частым тестированием и получением обратной связи.
*   **Минимально жизнеспособный продукт (MVP):** На каждом этапе будет реализовываться только самая необходимая функциональность, достаточная для проверки основной идеи или гипотезы. Избегаем преждевременной оптимизации и добавления избыточных функций.
*   **Простота и ясность:** Код должен быть максимально простым, понятным и легко поддерживаемым. Приоритет отдается читаемости и предсказуемости.
*   **CI/CD (в зачаточном состоянии):** На начальном этапе будет достаточно базовых проверок и автоматизации развертывания с помощью `make` для ускорения циклов обратной связи. Полноценные CI/CD пайплайны будут добавлены по мере роста проекта и необходимости.

## Структура проекта

Для обеспечения максимальной простоты и быстроты развертывания, структура проекта будет минималистичной и функционально-ориентированной:

*   **`main.py`**: Главный файл, точка входа в приложение. Инициализация бота, регистрация хэндлеров и запуск `polling`.
*   **`handlers.py`**: Модуль для всех обработчиков команд и сообщений Telegram-бота. Здесь будет находиться логика, отвечающая на взаимодействие пользователя.
*   **`llm_service.py`**: Модуль, инкапсулирующий логику взаимодействия с LLM (через `openai` клиент и Openrouter). Будет отвечать за формирование запросов, отправку их в LLM и обработку ответов.
*   **`config.py`**: Модуль для управления конфигурацией проекта (токены, API ключи, системные промпты и т.д.).
*   **`utils.py`**: Общие вспомогательные функции, которые могут быть использованы в разных частях проекта.
*   **`Makefile`**: Файл для автоматизации сборки, запуска и развертывания проекта.
*   **`requirements.txt`**: Список зависимостей проекта, генерируемый `uv`.

## Архитектура проекта

Архитектура проекта будет максимально простой и линейной, следуя принципам "один компонент - одна ответственность" для быстрого прототипирования и тестирования.

*   **Telegram Bot (aiogram):** Ядро бота, отвечающее за взаимодействие с Telegram API. Принимает сообщения от пользователей и отправляет им ответы.
*   **LLM Service:** Модуль, выступающий в роли адаптера для взаимодействия с различными LLM. На данный момент будет использоваться `openai` клиент через провайдера Openrouter. Он будет получать запросы от бота, форматировать их, отправлять в LLM и возвращать обработанные ответы.
*   **Конфигурация (Config):** Единая точка для хранения всех настроек приложения, включая токены, API ключи, системные промпты и другие параметры.

**Взаимодействие:**

Пользователь -> Telegram Bot -> LLM Service -> Openrouter (LLM) -> LLM Service -> Telegram Bot -> Пользователь

Это обеспечивает четкое разделение обязанностей и позволяет легко заменить или расширить любой компонент в будущем.

## Модель данных

На начальном этапе, для проверки идеи и сохранения максимальной простоты, модель данных будет максимально упрощена. Мы будем хранить только необходимую информацию для поддержания диалога с пользователем.

*   **Состояние диалога:** Будет храниться в оперативной памяти (например, в словаре или простом объекте) на время активной сессии пользователя. Это позволит отслеживать историю сообщений для поддержания контекста при обращении к LLM. Для MVP не требуется постоянное хранение истории диалогов. При перезапуске бота или завершении сессии диалог будет начинаться заново.
*   **Идентификатор пользователя Telegram:** Используется для идентификации пользователя и привязки к нему состояния диалога.
*   **Сообщения пользователя:** Текстовое содержимое сообщений от пользователя.
*   **Ответы LLM:** Текстовое содержимое ответов, полученных от LLM.

**Пример структуры данных (в памяти):**

```python
{
    "user_id_1": {
        "chat_history": [
            {"role": "user", "content": "Привет!"},
            {"role": "assistant", "content": "Здравствуйте, я ваш личный сомелье."},
            // ... более старые сообщения
        ]
    },
    "user_id_2": {
        "chat_history": [
            {"role": "user", "content": "Посоветуйте вино."},
            // ...
        ]
    }
}
```

Такой подход позволяет избежать использования баз данных и ORM на начальном этапе, что существенно упрощает разработку и развертывание.

## Работа с LLM

Взаимодействие с Large Language Model (LLM) будет осуществляться через `openai` клиент, настроенный на работу с провайдером Openrouter. Это позволит легко переключаться между различными моделями и провайдерами в будущем, при этом сохраняя унифицированный интерфейс.

*   **Использование `openai` клиента:** Для взаимодействия с LLM будет использоваться стандартный Python-клиент `openai`. Это обеспечивает привычный интерфейс и доступ к широкому спектру функций.
*   **Провайдер Openrouter:** В качестве провайдера LLM будет использоваться Openrouter, что дает гибкость в выборе моделей и, при необходимости, оптимизации затрат.
*   **Системный промпт:** В `llm_service.py` будет определен статический системный промпт, задающий роль LLM (например, "ты - опытный сомелье"). Этот промпт будет включаться в каждый запрос к LLM для поддержания необходимого контекста.
*   **История диалога:** История последних `N` сообщений пользователя и ответов LLM будет передаваться в каждом запросе к LLM для поддержания контекста диалога. Количество сообщений `N` будет конфигурируемым параметром. Для MVP этого достаточно, не требуется сложной обработки длинных диалогов или векторизации.
*   **Простые запросы:** Запросы к LLM будут максимально простыми, без использования сложных инструментов, функций или RAG на начальном этапе. Фокус на получении прямого ответа на основе контекста диалога и системного промпта.

## Сценарии работы

Основной сценарий работы бота будет максимально простым, сосредоточенным на интерактивном диалоге с пользователем через Telegram и получении ответов от LLM.

*   **Начало диалога (`/start`):**
    *   Пользователь отправляет команду `/start`.
    *   Бот приветствует пользователя и кратко описывает свою роль (например, "Я ваш личный сомелье, готов помочь с выбором вина").
    *   Инициируется новая сессия диалога для этого пользователя (или сбрасывается существующая).
*   **Ведение диалога:**
    *   Пользователь отправляет текстовое сообщение (например, "Посоветуй легкое белое вино к рыбе").
    *   Бот получает сообщение, сохраняет его в историю диалога для текущего пользователя.
    *   Бот формирует запрос к LLM, включая системный промпт и актуальную историю диалога.
    *   Отправляет запрос в LLM Service, который пересылает его в Openrouter.
    *   Получает ответ от LLM.
    *   Сохраняет ответ LLM в историю диалога.
    *   Отправляет ответ пользователю в Telegram.
*   **Продолжение диалога:**
    *   Пользователь продолжает задавать вопросы или уточнять предыдущий запрос.
    *   Бот повторяет шаги ведения диалога, используя обновленную историю для поддержания контекста.

На этом этапе мы не рассматриваем сложные сценарии, такие как обработка изображений, голосовых сообщений, сложных команд, интеграцию с внешними сервисами или персонализацию. Фокус на минимальной, но рабочей цепочке взаимодействия.

## Подход к конфигурированию

Для обеспечения максимальной простоты и удобства развертывания, особенно на начальных этапах, конфигурирование будет осуществляться с использованием переменных окружения и файла `.env`. Это позволит легко управлять секретами и настройками без изменения кода.

*   **Переменные окружения:** Все чувствительные данные, такие как токены Telegram бота, API ключи Openrouter, будут передаваться через переменные окружения. Это стандартный и безопасный подход для развертывания в различных окружениях (разработка, тестирование, продакшн).
*   **Файл `.env`:** Для локальной разработки и тестирования будет использоваться файл `.env`, в котором будут определены все необходимые переменные окружения. Библиотека `python-dotenv` будет использоваться для загрузки этих переменных в приложение.
*   **Модуль `config.py`:** Все настройки будут централизованно собираться в модуле `config.py`, который будет загружать переменные окружения и предоставлять к ним удобный доступ. В этом же модуле можно будет определить значения по умолчанию для некоторых параметров.
*   **Отсутствие сложных конфигурационных файлов:** На данном этапе не предполагается использование сложных форматов конфигурации (YAML, TOML) или конфигурационных серверов. Принципы KISS и YAGNI диктуют минималистичный подход.

**Пример файла `.env`:**

```
TELEGRAM_BOT_TOKEN="ВАШ_ТОКЕН_ТЕЛЕГРАМ"
OPENROUTER_API_KEY="ВАШ_API_КЛЮЧ_OPENROUTER"
LLM_MODEL="gpt-3.5-turbo"
LLM_SYSTEM_PROMPT="Ты - опытный сомелье, который помогает пользователям выбрать вино."
CHAT_HISTORY_LENGTH=5
```

## Подход к логгированию

Логгирование будет реализовано максимально просто, с использованием стандартных средств Python, чтобы обеспечить базовую видимость работы приложения без избыточной сложности.

*   **Стандартная библиотека `logging`:** Будет использоваться встроенная в Python библиотека `logging`. Это позволяет легко настроить уровни логгирования, форматы сообщений и обработчики (консоль, файл).
*   **Логгирование в консоль:** На начальном этапе все логи будут выводиться в консоль (stdout/stderr). Это удобно для локальной разработки и отладки, а также для контейнеризированных приложений.
*   **Уровни логгирования:** Будут использоваться стандартные уровни: `INFO` для общей информации о работе бота (старт, получение сообщения, отправка ответа), `WARNING` для потенциальных проблем и `ERROR` для критических ошибок. `DEBUG` можно использовать при необходимости более детальной отладки.
*   **Минимальный объем логов:** Логи будут содержать только самую необходимую информацию: время события, уровень лога, имя модуля и само сообщение. Избегаем логгирования чувствительных данных или избыточных деталей.
*   **Без внешних систем:** На данном этапе не предполагается интеграция со сложными системами сбора и анализа логов (ELK Stack, Grafana Loki и т.д.). Фокус на простой и эффективной отладке.
